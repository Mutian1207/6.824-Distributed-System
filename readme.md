# MIT 6.824 分布式系统笔记

## 核心思想

* 可扩展性(Scalability):关注系统性能,可以通过增加机器来提高系统性能
* 一致性(Consistency):关注数据一致性
  * 强一致性:为了保证数据一致性而牺牲系统性能
  * 弱一致性:为了提高系统性能而在一定程度上牺牲数据一致性
* 容错性(Fault Tolerance):系统能够容忍一定程度的故障
  * 可用性(Availability):通过非易失性存储和副本机制保证系统的可用性
  * 自愈性(Recoverability):系统能够从故障中恢复

## GFS (Google File System)

## Primary-Backup Replication(主从复制)

## Raft

### 设计架构

* Raft 以库的形式存在于服务中,上层应用程序通过调用Raft库的API来完成多副本数据同步
* 设计主要包括以下几大部分:
  1. Leader 选举机制
     * 初始时所有节点状态都为Follower,每个节点都有一个随机的选举超时时间,超时后会转变为Candidate并发起选举
     * 初始化时同时开启一个协程来检查是否超时,主要通过比较当前时间与上次收到RPC的时间差是否超过选举超时时间
     * 发起选举时,Candidate 会向其他所有节点并行发送RequestVote RPC,使用channel等待大多数节点的响应,并设置RPC超时时间。如果得到大多数节点的投票,则成为新的Leader
     * 成为Leader后自动在后台创建一个协程,定期向其他所有节点发送心跳以维持Leader权威
  2. 日志复制
     * Leader在本地添加新的日志条目后,会向其他每个节点创建一个协程来复制日志。复制是在客户端请求时触发的,无需后台定时检查。Leader为每个节点维护了一个nextIndex[]变量,指示下一个要发送的日志条目索引,这样可以处理并发和乱序的日志复制请求
     * Leader 只在大多数节点写入日志后,才提交日志条目。具体实现是,每次收到节点的复制成功响应后,更新matchIndex[],当大多数matchIndex[]≥n时,提交对应的日志条目log[n]
     * Follower 处理日志复制请求是在处理心跳的同时进行的。更新commitIndex以及将已提交日志应用到状态机,也是由心跳触发的(这里假设客户端请求不频繁)
     * Follower在复制日志前,需要进行一致性检查,如果出现不一致则返回false给Leader,Leader会递减nextIndex[]重试,直到与Follower的日志匹配
     * 日志复制时遵循以下原则:保留已经匹配的日志条目,只替换或追加冲突的日志条目以及新的日志条目。具体实现是找到第一个冲突的日志索引,如果未找到,则直接追加新的日志条目
     * 如果在日志复制过程中出现异常,可能导致Leader和Follower日志出现不一致。Leader会尝试探测Follower日志中第一个与自己匹配的位置,然后从这个位置开始重新发送后续的所有日志,最终使所有节点的日志趋于一致
     * 快速恢复(未实现)
       * 如果Leader和Follower日志出现冲突,Leader采用一种一步一步递减nextIndex[]的方式进行回退。如果冲突日志过多,通过多轮RPC交互会影响系统性能
       * 可以让Follower在响应中返回冲突位置附近的任期信息,Leader就可以直接越过这个冲突任期,而无需逐步回退,加速日志恢复速度
  3. 持久化
     * 为了在节点宕机后快速恢复状态,以下三个关键变量需要持久化到非易失存储:
       * Log:记录了状态机的所有状态变更,新选出的Leader需要知道哪些日志已经被提交
       * currentTerm:单调递增,保证每个任期最多只有一个Leader  
       * votedFor:记录当前任期投票给了哪个节点,每个节点在一个任期内最多只能投票一次,以避免出现多个Leader
     * 我主要使用Go语言的gob库进行编码和解码,以字节流的形式将上述三个关键变量写入磁盘。每次变量更新时进行持久化
     * Q:为什么commitIndex、lastApplied、nextIndex[]、matchIndex[]可以不进行持久化?
       * A:这是Raft算法在权衡简洁性和安全性后做出的设计选择。因为Leader可以通过自身的日志以及AppendEntries RPC的响应来重建这些变量,虽然会影响系统可用性,但不影响安全性。
  4. 日志压缩和快照
     * 随着系统的运行,日志会无限增长,我们需要周期性地对日志进行压缩
     * 主要思路是Leader定期将当前全部已提交日志应用到状态机,得到一个完整的系统状态快照,然后把快照之前的所有日志条目都丢弃
     * 具体实现是,Leader在达到日志压缩触发条件时,通知所有节点安装快照。节点收到安装快照的RPC后,如果发现自己的日志落后,则直接用快照替换本地状态,并丢弃所有旧日志
     * 安装快照后,Raft需要使用快照最后包含的日志条目的索引和任期来初始化自己的Log对象,以便后续的日志复制可以基于快照状态进行
     * 同时还要注意一些边界条件,比如如果Leader向Follower发送的快照已经被压缩,需要通过减小快照间隔的方式来避免

### 一些问题的思考

* Q:为什么需要Leader?
  * A:Leader可以保证所有的副本按照相同的顺序执行相同的操作,简化了系统设计。但Leader并不是必须的,比如Paxos算法就没有Leader的概念,只要有超过半数的节点响应就可以认为达成一致。但在没有故障的情况下,Leader的存在可以使系统更高效。
* Q:为什么需要Term?
  * A:Term可以用来区分Leader的新旧,节点可以根据Term决定是否接受Leader的日志复制请求。另外,Leader在给定Term内才有日志复制的权威,如果发现有更大的Term,要主动让出Leadership并回滚本Term内提交的日志。
* 选举约束
  * 只投票给拥有更高任期号的Candidate,以避免产生多个分区的Leader
  * 在选票被瓜分导致选举失败的情况下,优先投票给日志更加完整的Candidate,以尽快使系统恢复可用
    * 但论文中并未实现,而是通过随机超时时间来错开选举,避免长期选票被瓜分
* Q:为什么不直接选举拥有最长日志的节点作为Leader?
  * A:因为在选举过程中,节点无法直接比较彼此的日志长度。强一致的方案需要先选出一个节点收集所有节点的日志长度,在网络分区的情况下,这会导致可用性下降。Raft通过多数投票来规避这个问题,用业务延迟换取了可用性。
* 选举定时器
  * 每个节点都有一个选举定时器,在超时时间内如果没有收到Leader的心跳,则认为Leader已经失效,自增当前任期号并发起新一轮选举
    * 虽然可能会不必要地发起选举,但这可以保证系统的活性和安全性
    * 发起选举并不代表之前的Leader一定失效了,也有可能是因为网络原因没收到Leader的心跳
* 随机选举超时时间
  * Raft采用随机的选举超时时间来尽量避免多个节点同时发起选举导致选票被瓜分的情况
  * 超时时间的下限要大于Leader发送心跳的间隔,上限要足够长以便完成一次选举,同时要尽量短以减少系统不可用时间
* Q:节点如何感知到新Leader产生?
  * A:新Leader通过选举产生后,会给所有节点发送包含新任期号的心跳,其他节点收到后就知道新Leader已经产生
* Q:Raft如何保证不会产生脑裂?
  * A:多数投票机制可以避免网络分区后产生多个Leader,从而避免数据和状态的不一致。任何一个分区要想选出新的Leader,必须包含超过半数的节点,而系统不可能同时存在两个这样的分区。

* 具体实现细节参考 <http://nil.csail.mit.edu/6.824/2020/papers/raft-extended.pdf>